## Chunking, lancedb, vector database

In this project I will be using a lancedb's vector search to find the right chunks of text that will be added to a prompt as a context to answer questions generated by the same llm. 
The llm will use the given context to answer the respective question. The questions will be generated by the llm, by giving it a chunk and asking it to generate a question.
The idea of this project is to get an idea of what rag is, in practice however it could look a lot different. 

Some flaws still to tackle: 

- the input length of the SentenceTransformer is actuall too small to capture some of the longer summaaries.
- there are PDFs containing tables, these should be extracted separately and saved like the chunks.

### Chunking

In this project I am using semchunk to convert a some relatively simple PDFs into text chunks. 
After chunking chunks are serialized using a pickle file.

```
def semchunking(files):
    final_chunks={}
    for file in files:
        print(file)
        loader = PyPDFLoader(file, mode="single")
        pages = [page.page_content for page in loader.load()]

        chunker = semchunk.chunkerify("cl100k_base", 256)
        chunks=chunker(pages[0].replace("\n",""))
        chunk_number = 1
        for c in chunks:
            key = (file, chunk_number)
            final_chunks[key] = c
            chunk_number=chunk_number+1

    with open("data/out/pkl/final_chunks_sl.pkl", "wb") as f:
        pickle.dump(final_chunks, f)
```

### Summarizing

As I will be using the summary of a chunk as an index to retrieve the appropriate chunks, I need an llm to generate a summary for each chunk.
For this I will use the llm. The call is made using the code below.

```
def invoke_ai(system_message: str, context: str) -> str:
    client = OpenAI(base_url="http://*.*.*.*:port/v1", api_key="lm-studio")
    response = client.chat.completions.create(
        #model="o4-mini",
        model="gemma-1.1-2b-it",
        messages=[
            {"role": "system", "content": system_message+context},
        ],
    )
    return response.choices[0].message.content
```

The prompt to generate the summary looks like this. 

```
"Summarize the following text in a commercial way. Focus on facts, ideas used. Add a fitting title to the summary. Be impersonal.
Context:
```

As I am running my LLM locally I am using a simple LLM, a call to this LLM takes several minutes.

### Generate Q&As

To generate a question and an answer for every chunk, I have forced the call to the LLM to return json.
First I define the pydantic object.

```
class Record(BaseModel):
    question: str
    answer: str
```

The object will be converted to model_json_schema() in the following piece of code.

```
def invoke_ai_json(system_message: str, context: str) -> str:
    """
    Generic function to invoke an AI model given a system message and context. 
    The OpenAI response is a json object.
    """
    client = OpenAI(base_url="http://192.168.178.66:1234/v1", api_key="lm-studio")
    response = client.chat.completions.create(
        model="gemma-1.1-2b-it",
        response_format={
            "type":"json_schema",
            "json_schema":{
                "name": "output_schema",
                "schema": Record.model_json_schema()
            }
        },
        messages=[
            {"role": "system", "content": system_message + context},
        ],
    )
    return response.choices[0].message.content
```

The returned json looks like this for some chunk.

```
{"question": "What was the most significant factor contributing to the downfall of Swan Lagoon?",
 "answer": "The decline in gold yields by the early 1920s."}
```

### LanceDB

LanceDB is an embedded database that persists vectorial and non-vectorial data. In another context this could just as well be a more globally available service. 
In this case I will use LanceDB to store vectorrized chunk summaries to a vector, to store in a lancedb. Some other fields will be used to.

```
self.vector_db = lancedb.connect("data/rag-lancedb")
schema = pa.schema(
            [
                pa.field('id', pa.int32()),
                pa.field("summary_vector", pa.list_(pa.float32(), self.vector_dimensions)),
                pa.field("content", pa.utf8()),
                pa.field("source", pa.utf8()),
                pa.field("summary", pa.utf8()),
                pa.field("number", pa.utf8()),
                pa.field("question", pa.utf8()),
                pa.field("answer", pa.utf8())
            ]
        )

self.vector_db.create_table("rag_table", schema=schema)
```
As can be seen from the code above, the vector field is present and some additional fields, like the question and matching answer field. Also the source field representing the name of the pdf and the chunk number.

To convert a string to a vector I will use a SentenceTransformer. This model takes in some maximum numbers of tokens and converts this to a fixed length vector.

The vector can be used to perform a semantic search. By applying some simple vector arithmetic to the embeddings, lanceDB allows us to manipulate the embeddings in ways that capture complex relationships and nuances in the data.
 
To get the vector the following code is used. The emb_model is the SentenceTransformer.

```
def get_vector(self, summary: str) -> List[float]:
    if not summary.strip():
        print("Attempted to get embedding for empty text.")
        return []

    embedding = self.emb_model.encode(summary)
    l = embedding.tolist()
    return l
```

Whenever a question needs the appropriate chunk, the question can be vectorized using the get_vector method, after it will be used in a query like below.
This code return some number of chunks (content, summary, question, number, source pdf).

```
def search(self, query: str, top_k: int = 5) -> list[dict]:
    vector = self.get_vector(query)
        results = (
            self.table.search(vector)
            .select(["summary", "content","source", "question", "number"])
            .limit(top_k)
            .to_list()
        )
        return results
```

The result of this query can be seen in this table.

<img width="1028" height="108" alt="datastore_results_2" src="https://github.com/user-attachments/assets/60f7da3c-fe3f-4342-a77a-2932aab09aa9" />



One can also see that all of the results of the vector search are in the right pdf, but only one search result has the correct chunk number. But this result is the furthest away. 
The distance says something about the semantic distance between the correct vectorized summary and the vectorized question.

#### Reranking

The idea behind reranking is to use an LLM to see which chunk is the best chunk to answer the question.

So after getting back chunks to answer the question, an LLM sorts the chunks on relevance to answer the question.  
A simple way to do this is through an existing reranking service like cohere (not entirely free).
The reranked document(s) can then be passed on to the LMM to answer the question.

```
def _rerank(
    self, query: str, search_results: list[dict], top_k: int = 10
) -> list[dict]:
    result_content = [d["content"] for d in search_results]

    co = cohere.ClientV2(api_key=cohere_key)
    response = co.rerank(
        model="rerank-v3.5",
        query=query,
        documents=result_content,
        top_n=top_k,
    )

    result_indices = [result.index for result in response.results]
    print(f"âœ… Reranked Indices: {result_indices}")
    return [search_results[i] for i in result_indices]
```

As there are multiple ways to do index searching, it would be interesting to adapt the vector search based on the cohere results.

### Find an answer

The chunks being retrieved when vector searching the LanceDB datastore are used to feed to the LLM together with the question, at this moment the LLM should be able to generate a correct answer, that is if the returned chunks are correct.

```
def generate_response(self, query: str, context: List[str]) -> str:
    """Generate a response using OpenAI's chat completion."""
    # Combine context into a single string
    context_text = "\n".join(context)
    context_question = (
        f"<context>\n{context_text}\n</context>\n"
        f"<question>\n{query}\n</question>"
    )
    return invoke_ai(system_message=SYSTEM_PROMPT, context=context_question)
```

### Evaluate an answer

To get an answer the question, response and expected answer is send to the LLM for evaluation.
It returns a json object containing two fields an is_correct field and a reasoning field. 

```
def _evaluate(
        self, query: str, response: str, expected_answer: str
) -> str:
    context = f"""
    <question>\n{query}\n</question>
    <response>\n{response}\n</response>
    <expected_answer>\n{expected_answer}\n</expected_answer>
    """

    response_content = invoke_ai_json(system_message=SYSTEM_PROMPT, context=context, ret_object=AnswerEvaluation.model_json_schema())

    return response_content
```
<img width="1790" height="168" alt="qa" src="https://github.com/user-attachments/assets/b4f21257-10c5-4a9d-9e3b-5ab395873151" />

### LM studio

LM studio is a good application to serve an LLM locally. 
